{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec312f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f0a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data_csv\"\n",
    "RESULTS_DIR = \"results_csv\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd38e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dataset(file_name):\n",
    "    path = os.path.join(DATA_DIR, file_name)\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.drop(columns=[\"label\"]).values\n",
    "    y = df[\"label\"].values.astype(int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60970641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypothesis:\n",
    "    def __init__(self, w, b):\n",
    "        self.w = np.asarray(w, dtype=float)\n",
    "        self.b = float(b)\n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        scores = X.dot(self.w) + self.b\n",
    "        return np.where(scores >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4fc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_error(hyp, X, y):\n",
    "    if len(y) == 0:\n",
    "        return 0.5\n",
    "    preds = hyp.predict(X)\n",
    "    return float(np.mean(preds != y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbff6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vc_radius(n, dvc, delta):\n",
    "    if n <= 0:\n",
    "        return 1.0\n",
    "    term1 = dvc * np.log((2.0 * np.e * n) / max(dvc, 1.0))\n",
    "    term2 = np.log(4.0 / max(delta, 1e-12))\n",
    "    val = max((term1 + term2) / max(n, 1.0), 0.0)\n",
    "    return min(1.0, np.sqrt(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bfecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LB(hyp, X, y, dvc, delta_k):\n",
    "    n = len(y)\n",
    "    err_hat = empirical_error(hyp, X, y)\n",
    "    r = vc_radius(n, dvc, delta_k)\n",
    "    return max(0.0, err_hat - r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b40224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UB(hyp, X, y, dvc, delta_k):\n",
    "    n = len(y)\n",
    "    err_hat = empirical_error(hyp, X, y)\n",
    "    r = vc_radius(n, dvc, delta_k)\n",
    "    return min(1.0, err_hat + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da0192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_hypotheses(X, y, M=500, random_state=0):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n, d = X.shape\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "    hyps = []\n",
    "    n_boot = max(1, int(M/2))\n",
    "    for i in range(n_boot):\n",
    "        idx = rng.choice(n, size=n, replace=True)\n",
    "        clf = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=200)\n",
    "        try:\n",
    "            clf.fit(Xs[idx], y[idx])\n",
    "            w = clf.coef_.ravel()\n",
    "            b = clf.intercept_.item()\n",
    "            # unscale\n",
    "            w_un = w / scaler.scale_\n",
    "            b_un = b - np.dot(w, scaler.mean_/scaler.scale_)\n",
    "            hyps.append(LinearHypothesis(w_un, b_un))\n",
    "        except Exception:\n",
    "            hyps.append(LinearHypothesis(rng.randn(d), rng.randn()))\n",
    "    for _ in range(M - n_boot):\n",
    "        hyps.append(LinearHypothesis(rng.randn(d), rng.randn()))\n",
    "    return hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa02ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_disagreement_fraction(H, X_subset):\n",
    "    if X_subset is None or len(X_subset) == 0:\n",
    "        return 0.0\n",
    "    preds = np.array([h.predict(X_subset) for h in H])  # shape (|H|, m)\n",
    "    disag = np.sum(preds.max(axis=0) != preds.min(axis=0))\n",
    "    return float(disag) / float(X_subset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb580a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_csv(filepath, dataset_name, method_name, n_labels, accuracy, total_time, learning_curve):\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    lc_str = \";\".join(f\"{int(n)}:{float(a)}\" for n, a in learning_curve)\n",
    "    file_exists = os.path.isfile(filepath)\n",
    "    with open(filepath, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"dataset\", \"method\", \"n_labels\", \"accuracy\", \"total_time\", \"learning_curve\"])\n",
    "        writer.writerow([dataset_name, method_name, n_labels, float(accuracy), float(total_time), lc_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c4e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2_algorithm1(X_pool, Y_pool, H, dvc, epsilon=0.05, delta=0.05,\n",
    "                  max_outer=100, rng_seed=0, save_path=None, dataset_name=\"undefined\"):\n",
    "    \"\"\"\n",
    "    Implémentation fidèle de l'Algorithm 1 A^2 (selon ton pseudo-code collé).\n",
    "    Paramètres:\n",
    "        X_pool, Y_pool : pool simulant D (entièrement visible pour la simulation)\n",
    "        H : classe d'hypothèses approximée par une liste finie (version space approx)\n",
    "        dvc : VC-dimension (ex: d+1 pour séparateurs linéaires)\n",
    "        epsilon, delta : paramètres\n",
    "    Retour :\n",
    "        best_h, log\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    n_total = X_pool.shape[0]\n",
    "    pool_indices = np.arange(n_total)\n",
    "\n",
    "    # Initialize variables as in the pseudo-code\n",
    "    i = 1\n",
    "    # D_i simulated by the empirical pool (we'll restrict indices when needed)\n",
    "    Di_indices = pool_indices.copy()\n",
    "    Hi = list(H)\n",
    "    Si_x = np.zeros((0, X_pool.shape[1]))\n",
    "    Si_y = np.zeros((0,), dtype=int)\n",
    "    k = 1\n",
    "\n",
    "    learning_curve = []\n",
    "    n_labels = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # helper to get min UB and min LB over a set of hypotheses Hset with current Si and δ_k\n",
    "    def min_UB_and_min_LB(Hset, Sx, Sy, dvc, delta_k):\n",
    "        if len(Hset) == 0:\n",
    "            return 1.0, 0.0\n",
    "        UB_vals = np.array([UB(h, Sx, Sy, dvc, delta_k) for h in Hset])\n",
    "        LB_vals = np.array([LB(h, Sx, Sy, dvc, delta_k) for h in Hset])\n",
    "        return float(np.min(UB_vals)), float(np.min(LB_vals))\n",
    "\n",
    "    # Outer while condition:\n",
    "    # while Disagree_D(Hi) * (min_{h in Hi} UB(Si, h, δ^k) - min_{h in Hi} LB(Si, h, δ^k)) > epsilon\n",
    "    while True:\n",
    "        # compute current disagreement fraction on Di_indices (simulate Disagree_D(Hi) using Di)\n",
    "        if len(Di_indices) == 0:\n",
    "            disagree_Hi = 0.0\n",
    "        else:\n",
    "            disagree_Hi = empirical_disagreement_fraction(Hi, X_pool[Di_indices, :])\n",
    "\n",
    "        # compute delta_k for current k; we choose δ_k = delta / k (explicit choice)\n",
    "        delta_k = max(delta / max(1, k), 1e-12)\n",
    "        min_UB_Hi, min_LB_Hi = min_UB_and_min_LB(Hi, Si_x, Si_y, dvc, delta_k)\n",
    "\n",
    "        outer_metric = disagree_Hi * (min_UB_Hi - min_LB_Hi)\n",
    "        # If outer_metric <= epsilon -> stop and return best hypothesis\n",
    "        if outer_metric <= epsilon:\n",
    "            # choose h = argmin_{h in Hi} UB(Si, h, δ_k)\n",
    "            UB_vals = np.array([UB(h, Si_x, Si_y, dvc, delta_k) for h in Hi]) if len(Hi)>0 else np.array([1.0])\n",
    "            best_idx = int(np.argmin(UB_vals))\n",
    "            best_h = Hi[best_idx] if len(Hi)>0 else H[0]\n",
    "            total_time = time.time() - start_time\n",
    "            best_h.n_labels_used = n_labels\n",
    "            best_h.learning_curve = learning_curve\n",
    "            if save_path is not None:\n",
    "                preds = best_h.predict(X_pool)\n",
    "                acc = float(np.mean(preds == Y_pool))\n",
    "                save_results_csv(save_path, dataset_name, \"A2_algorithm1\", n_labels, acc, total_time, learning_curve)\n",
    "            return best_h, {\"n_labels\": n_labels, \"learning_curve\": learning_curve, \"total_time\": total_time}\n",
    "\n",
    "        # Step 1 of pseudo-code: set Si = ∅, H0_i = Hi, k <- k + 1\n",
    "        Si_x = np.zeros((0, X_pool.shape[1]))\n",
    "        Si_y = np.zeros((0,), dtype=int)\n",
    "        H0_i = list(Hi)\n",
    "        k += 1  # increment as in the pseudocode\n",
    "\n",
    "        # precompute Disagree_D(Hi) for inner loop threshold\n",
    "        threshold_half = 0.5 * disagree_Hi\n",
    "\n",
    "        # Inner loop: while Disagree_D(H0_i) >= 1/2 Disagree_D(Hi)\n",
    "        # use Di (current Di indices from previous iteration) to measure disagreement\n",
    "        # We'll use the full pool to get fraction for H0_i (consistent with simulation)\n",
    "        while True:\n",
    "            disagree_H0i = empirical_disagreement_fraction(H0_i, X_pool[Di_indices, :]) if len(Di_indices)>0 else 0.0\n",
    "            if disagree_H0i < threshold_half:\n",
    "                break  # inner while condition false -> exit inner loop\n",
    "\n",
    "            # (a) check if Disagree_D(H0_i) * (min_{h in H0_i} UB - min_{h in H0_i} LB) <= epsilon\n",
    "            # use δ_k in UB/LB\n",
    "            delta_k = max(delta / max(1, k), 1e-12)\n",
    "            min_UB_H0i, min_LB_H0i = min_UB_and_min_LB(H0_i, Si_x, Si_y, dvc, delta_k)\n",
    "            inner_metric = disagree_H0i * (min_UB_H0i - min_LB_H0i)\n",
    "\n",
    "            if inner_metric <= epsilon:\n",
    "                # (b) return h = argmin_{h in H0_i} UB(Si, h, δ_k)\n",
    "                UB_vals = np.array([UB(h, Si_x, Si_y, dvc, delta_k) for h in H0_i]) if len(H0_i)>0 else np.array([1.0])\n",
    "                best_idx = int(np.argmin(UB_vals))\n",
    "                best_h = H0_i[best_idx] if len(H0_i)>0 else H[0]\n",
    "                total_time = time.time() - start_time\n",
    "                best_h.n_labels_used = n_labels\n",
    "                best_h.learning_curve = learning_curve\n",
    "                if save_path is not None:\n",
    "                    preds = best_h.predict(X_pool)\n",
    "                    acc = float(np.mean(preds == Y_pool))\n",
    "                    save_results_csv(save_path, dataset_name, \"A2_algorithm1\", n_labels, acc, total_time, learning_curve)\n",
    "                return best_h, {\"n_labels\": n_labels, \"learning_curve\": learning_curve, \"total_time\": total_time}\n",
    "\n",
    "            # (c) else:\n",
    "            # i. S0_i = Rejection sample 2|Si| + 1 samples x from D such that exists disagreement in Hi\n",
    "            batch_size = int(2 * len(Si_y) + 1)\n",
    "            collected = 0\n",
    "            attempts = 0\n",
    "            max_attempts = max(1000, batch_size * 200)\n",
    "            # D_i for rejection criterion is based on Hi (not H0_i)\n",
    "            # compute disag mask for Hi over the full empirical pool\n",
    "            preds_Hi = np.array([h.predict(X_pool) for h in Hi])\n",
    "            disag_mask_Hi = (preds_Hi.max(axis=0) != preds_Hi.min(axis=0))\n",
    "            # Build list of candidate indices (Di_indices already simulate Di)\n",
    "            # But criterion requires \"exists h1,h2 in Hi : h1(x)!=h2(x)\" -> use disag_mask_Hi\n",
    "            candidate_indices = pool_indices[disag_mask_Hi]\n",
    "            if len(candidate_indices) == 0:\n",
    "                # nothing to sample; break inner loop to avoid infinite loop\n",
    "                break\n",
    "\n",
    "            while collected < batch_size and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                idx = int(rng.choice(pool_indices))\n",
    "                # Accept only if idx is candidate (disagreement under Hi)\n",
    "                if not disag_mask_Hi[idx]:\n",
    "                    continue\n",
    "                # otherwise accept this sample\n",
    "                x = X_pool[idx:idx+1]\n",
    "                y = np.array([Y_pool[idx]])\n",
    "                Si_x = np.vstack([Si_x, x]) if Si_x.size else x.copy()\n",
    "                Si_y = np.concatenate([Si_y, y]) if Si_y.size else y.copy()\n",
    "                collected += 1\n",
    "                n_labels += 1\n",
    "\n",
    "            # ii. Si ← Si ∪ {(x,O(x)) : x ∈ S0_i}; k ← k + 1;\n",
    "            k += 1\n",
    "\n",
    "            # iii. H0_i = {h ∈ Hi : LB(Si, h, δ_k) ≤ min_{h0 ∈ Hi} UB(Si, h0, δ_k)} , k ← k + 1.\n",
    "            delta_k = max(delta / max(1, k), 1e-12)\n",
    "            UB_vals = np.array([UB(h, Si_x, Si_y, dvc, delta_k) for h in Hi])\n",
    "            LB_vals = np.array([LB(h, Si_x, Si_y, dvc, delta_k) for h in Hi])\n",
    "            min_UB_val_over_Hi = float(np.min(UB_vals)) if len(UB_vals)>0 else 1.0\n",
    "\n",
    "            H0_i_new = []\n",
    "            for idx_h, h in enumerate(Hi):\n",
    "                if LB_vals[idx_h] <= min_UB_val_over_Hi + 1e-12:\n",
    "                    H0_i_new.append(h)\n",
    "            if len(H0_i_new) > 0:\n",
    "                H0_i = H0_i_new\n",
    "            # increment k again per pseudo-code\n",
    "            k += 1\n",
    "\n",
    "            # update disagree_H0i for next inner loop check (we'll recompute at top)\n",
    "            # but to keep consistent recompute Di_indices as restriction of Di by H0_i\n",
    "            # Di is Di_indices restricted to points where H0_i disagree\n",
    "            if len(Di_indices) > 0 and len(H0_i) > 0:\n",
    "                preds_H0i = np.array([h.predict(X_pool) for h in H0_i])\n",
    "                disag_mask_H0i = (preds_H0i.max(axis=0) != preds_H0i.min(axis=0))\n",
    "                Di_indices = Di_indices[disag_mask_H0i[Di_indices]] if len(Di_indices)>0 else Di_indices\n",
    "\n",
    "            # update learning curve with current best hypothesis according to UB on Si\n",
    "            if len(H0_i) > 0:\n",
    "                delta_k_now = max(delta / max(1, k), 1e-12)\n",
    "                UB_vals_now = np.array([UB(h, Si_x, Si_y, dvc, delta_k_now) for h in H0_i])\n",
    "                best_tmp = H0_i[int(np.argmin(UB_vals_now))]\n",
    "                acc_tmp = float(np.mean(best_tmp.predict(X_pool) == Y_pool))\n",
    "                learning_curve.append((n_labels, acc_tmp))\n",
    "\n",
    "        # End inner while\n",
    "        # 3. Hi+1 ← H0_i ; Di+1 ← Di conditioned on the disagreement ; i ← i + 1\n",
    "        Hi = list(H0_i)\n",
    "        # Di_indices is already restricted above; if not, restrict now by Hi\n",
    "        if len(Di_indices) > 0 and len(Hi) > 0:\n",
    "            preds_Hi = np.array([h.predict(X_pool) for h in Hi])\n",
    "            disag_mask_Hi = (preds_Hi.max(axis=0) != preds_Hi.min(axis=0))\n",
    "            Di_indices = pool_indices[disag_mask_Hi]\n",
    "        i += 1\n",
    "        # loop back to outer while condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de61ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running exact A^2 algorithm on synthetique.csv (n=200, d=2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m H_pool = generate_linear_hypotheses(X, y, M=\u001b[32m200\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     16\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m best_h, log = \u001b[43mA2_algorithm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdvc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdvc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mmax_outer\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m                            \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m t1 = time.time()\n\u001b[32m     22\u001b[39m acc = \u001b[38;5;28mfloat\u001b[39m(np.mean(best_h.predict(X) == y))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mA2_algorithm1\u001b[39m\u001b[34m(X_pool, Y_pool, H, dvc, epsilon, delta, max_outer, rng_seed, save_path, dataset_name)\u001b[39m\n\u001b[32m    129\u001b[39m y = np.array([Y_pool[idx]])\n\u001b[32m    130\u001b[39m Si_x = np.vstack([Si_x, x]) \u001b[38;5;28;01mif\u001b[39;00m Si_x.size \u001b[38;5;28;01melse\u001b[39;00m x.copy()\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m Si_y = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSi_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m Si_y.size \u001b[38;5;28;01melse\u001b[39;00m y.copy()\n\u001b[32m    132\u001b[39m collected += \u001b[32m1\u001b[39m\n\u001b[32m    133\u001b[39m n_labels += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "datasets = [\"synthetique.csv\", \"iris.csv\"]\n",
    "results_csv = os.path.join(RESULTS_DIR, \"A2_algorithm1_results.csv\")\n",
    "\n",
    "for dataset_file in datasets:\n",
    "    if not os.path.exists(os.path.join(DATA_DIR, dataset_file)):\n",
    "        print(f\"Dataset {dataset_file} missing in {DATA_DIR} – skipping.\")\n",
    "        continue\n",
    "\n",
    "    X, y = load_csv_dataset(dataset_file)\n",
    "    d = X.shape[1]\n",
    "    dvc = d + 1\n",
    "\n",
    "    print(f\"\\nRunning exact A^2 algorithm on {dataset_file} (n={len(y)}, d={d})\")\n",
    "    H_pool = generate_linear_hypotheses(X, y, M=200, random_state=42)\n",
    "\n",
    "    t0 = time.time()\n",
    "    best_h, log = A2_algorithm1(X, y, H_pool, dvc=dvc,\n",
    "                                epsilon=0.02, delta=0.05,\n",
    "                                max_outer=50, rng_seed=42,\n",
    "                                save_path=results_csv, dataset_name=dataset_file.replace(\".csv\",\"\"))\n",
    "    t1 = time.time()\n",
    "    acc = float(np.mean(best_h.predict(X) == y))\n",
    "    print(f\"Done: n_labels={log['n_labels']}, accuracy={acc:.4f}, time={t1-t0:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
