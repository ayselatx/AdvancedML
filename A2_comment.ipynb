{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62fcb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8259a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_schedule(n):\n",
    "    ### Definition : \n",
    "    # Î”â‚™ : la marge de confiance nÃ©cessaire pour infÃ©rer un label sans lâ€™oracle.\n",
    "    # Dans AÂ², Î”â‚™ contrÃ´le la confiance dans la comparaison des erreurs empiriques.\n",
    "\n",
    "        # Si : |err_pos - err_neg| > Î”â‚™ \n",
    "        #    alors lâ€™algorithme considÃ¨re que lâ€™un des deux labels est trÃ¨s probablement correct.\n",
    "        # Si la diffÃ©rence est trop faible, \n",
    "        #    alors il existe trop dâ€™ambiguÃ¯tÃ© â†’ requÃªte Ã  lâ€™oracle.\n",
    "\n",
    "    ### Fonction : \n",
    "    # Î”â‚™ = sq_rt ( 1 / n) * ð›¿(n)\n",
    "    # oÃ¹ Î´(n) est une fonction dÃ©croissante \n",
    "    # ( d'habitude : 1 / sq_rt(n), c / sq_rt(n), sq_rt((log(n)) / n )\n",
    "    # ici :  c / sq_rt(n) avec c = 0.1\n",
    "        \n",
    "    ### Interpretation:\n",
    "    # Lorsque n est petit â†’ Î”â‚™ est grand \n",
    "    #   â†’ lâ€™algorithme prÃ©fÃ¨re poser des questions Ã  lâ€™oracle.\n",
    "    # Lorsque n augmente â†’ Î”â‚™ diminue \n",
    "    #   â†’ lâ€™algorithme devient plus confiant et infÃ¨re plus de labels.\n",
    "    \n",
    "    return 0.1 / math.sqrt(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f8a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_A2(X, y, test_size=0.3, stream_seed=1):\n",
    "    \"\"\"\n",
    "    OUTPUT : \n",
    "    X_stream : flux pour lâ€™apprentissage actif (DonnÃ©es arrivant sÃ©quentiellement pour AÂ²)\n",
    "    X_pool : donnÃ©es candidates pour le flux (stream), utilisÃ©es pour lâ€™apprentissage actif.\n",
    "    X_test : donnÃ©es finales utilisÃ©es uniquement pour Ã©valuer le classifieur appris.\n",
    "    y_pool, y_test : labels correspondants.\n",
    "    X_pool_arr, y_pool_arr : copies utiles pour des comparaisons externes.\n",
    "    oracle : fonction simulant les \"questions au professeur\" (Donne le vrai label quand AÂ² ne peut pas dÃ©cider)\n",
    "\n",
    "\n",
    "                    DonnÃ©es globales X\n",
    "                             â”‚\n",
    "                             â–¼\n",
    "                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                      â”‚   X_pool   â”‚  â† donnÃ©es candidates\n",
    "                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "               â–¼                          â–¼\n",
    "           X_stream                       X_test\n",
    "      (flux pour AÂ²)                    (jamais vu)\n",
    "\n",
    "            AÂ² lit X_stream 1 par 1 :\n",
    "            - soit il infÃ¨re             â†’ ajoute Ã  S\n",
    "            - soit il demande Ã  lâ€™oracle â†’ ajoute Ã  T\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### STEP 0 :  Verifie donnÃ©es sont en bonnes format\n",
    "    y = np.array(y)\n",
    "\n",
    "    ### STEP 1 : SÃ©paration pool/test \n",
    "    # Dans lâ€™apprentissage actif, il nâ€™y a pas de X_train au dÃ©part.\n",
    "    # Le modÃ¨le construit progressivement son ensemble dâ€™entraÃ®nement en choisissant des points dans X_pool\n",
    "    # (stratify=y garantit que les proportions des classes sont respectÃ©es)\n",
    "    X_pool, X_test, y_pool, y_test = train_test_split( X, y, test_size=test_size, random_state=42, stratify=y    )\n",
    "\n",
    "    ### STEP 2 : Construction du flux de donnÃ©es (stream)\n",
    "    # le flux dans lequel AÂ² voit les points un par un\n",
    "    rng = np.random.RandomState(stream_seed)\n",
    "    indices = np.arange(len(X_pool))\n",
    "    rng.shuffle(indices)                # mÃ©lange les indices\n",
    "    X_stream = X_pool[indices]          # flux final (les donnÃ©es du pool dans un ordre alÃ©atoire)\n",
    "\n",
    "    ### STEP 3 : Conversion des labels du pool en {-1, +1}\n",
    "    y_pool_mapped = np.where(y_pool == 0, -1, 1) # traduit les labels initiaux ( 0 1) en labels conformes Ã  la thÃ©orie (-1 +1)\n",
    "\n",
    "    ### STEP 4 : Copies utiles des tableaux (pour comparaisons)\n",
    "    X_pool_arr = np.array(X_pool)\n",
    "    y_pool_arr = np.array(y_pool)\n",
    "\n",
    "    ### STEP 5 : \n",
    "    # Lâ€™oracle doit fournir le vrai label dâ€™un point du flux lorsque AÂ² le demande.\n",
    "    # Pour cela, on crÃ©e une table de correspondance :\n",
    "    #   clÃ© : un point (transformÃ© en tuple pour Ãªtre hashable)  \n",
    "    #   valeur : son label {âˆ’1, +1}\n",
    "\n",
    "    pool_map = {tuple(x): label for x, label in zip(X_pool, y_pool_mapped)}\n",
    "\n",
    "    def oracle(x):\n",
    "        # Si AÂ² ne peut pas dÃ©cider entre +1 et âˆ’1 pour un point, il interroge lâ€™oracle.\n",
    "        # Lâ€™oracle retourne le vrai label Ã  partir du dictionnaire pool_map.\n",
    "        ## Le +1 par dÃ©faut est un choix de sÃ©curitÃ© (mais rarement utilisÃ©, car tous les points du flux appartiennent au pool)\n",
    "        return pool_map.get(tuple(x), +1)\n",
    "\n",
    "    return {\n",
    "        \"X_stream\": X_stream,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"X_pool_arr\": X_pool_arr,\n",
    "        \"y_pool_arr\": y_pool_arr,\n",
    "        \"oracle\": oracle\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb29133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_queries(X_stream, X_test, y_test, oracle, learner):\n",
    "    accuracies = []\n",
    "    queries = []\n",
    "    S, T = [], []\n",
    "\n",
    "    y_test_mapped = np.where(y_test == 0, -1, 1)\n",
    "\n",
    "    for n, x_n in enumerate(X_stream, start=1):\n",
    "        h_pos = learner.learn_H(S + [(x_n, +1)], T)\n",
    "        h_neg = learner.learn_H(S + [(x_n, -1)], T)\n",
    "        err_pos = learner.empirical_error(h_pos, S + T)\n",
    "        err_neg = learner.empirical_error(h_neg, S + T)\n",
    "        Delta = learner.delta_schedule(max(1, n - 1))\n",
    "\n",
    "        inferred_label = None\n",
    "        if err_neg - err_pos > Delta:\n",
    "            inferred_label = +1\n",
    "        elif err_pos - err_neg > Delta:\n",
    "            inferred_label = -1\n",
    "\n",
    "        if inferred_label is not None:\n",
    "            S.append((x_n, inferred_label))\n",
    "        else:\n",
    "            y_n = oracle(x_n)\n",
    "            T.append((x_n, y_n))\n",
    "\n",
    "        # recalculer prÃ©cision toutes les 10 requÃªtes\n",
    "        if n % 10 == 0:\n",
    "            h_final = learner.learn_H(S, T)\n",
    "            preds = h_final.predict(X_test)\n",
    "            acc = np.mean(preds == y_test_mapped)\n",
    "            accuracies.append(acc)\n",
    "            queries.append(len(T))\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(queries, accuracies, marker='o')\n",
    "    plt.xlabel(\"Nombre de requÃªtes Ã  l'oracle\")\n",
    "    plt.ylabel(\"PrÃ©cision sur X_test\")\n",
    "    plt.title(\"Ã‰volution de la prÃ©cision vs nombre de requÃªtes\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70387c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(res, dataset_name):\n",
    "    labels = [\"AÂ² Active\", \"Passive baseline\"] + list(res[\"passive_classifiers\"].keys())\n",
    "    accs = [res[\"acc_active\"], res[\"acc_passive\"]] + list(res[\"passive_classifiers\"].values())\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.bar(labels, accs, color='skyblue')\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.title(f\"{dataset_name} â€” Test Accuracy Comparison\")\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a773c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2ActiveLearner:\n",
    "    \"\"\"\n",
    "    DonnÃ©es Internes:\n",
    "    S : (x, y_hat)  ensemble des points Ã©tiquetÃ©s implicitement (labels infÃ©rÃ©s par lâ€™algorithme ie ensemble des labels infÃ©rÃ©s, utilisÃ©s comme si lâ€™oracle avait rÃ©pondu).\n",
    "    T : (x, y_true) ensemble des points Ã©tiquetÃ©s explicitement (requÃªtes au vÃ©ritable oracle ie ensemble des labels rÃ©ellement demandÃ©s Ã  lâ€™oracle).\n",
    "    n_requetes_oracle : nombre total de requÃªtes Ã  lâ€™oracle.\n",
    "\n",
    "    DonnÃ©es Externes:\n",
    "    delta_schedule : Î”â‚™ (fonction)  la marge de confiance nÃ©cessaire pour infÃ©rer un label sans lâ€™oracle.\n",
    "    learn_H :\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learn_H, delta_schedule):\n",
    "        ## DonnÃ©es internes : \n",
    "        self.S = []\n",
    "        self.T = []\n",
    "        self.n_requetes_oracle = 0\n",
    "\n",
    "        self.learn_H = learn_H\n",
    "        self.delta_schedule = delta_schedule\n",
    "\n",
    "    @staticmethod\n",
    "    def empirical_error(h, dataset):\n",
    "        if len(dataset) == 0:\n",
    "            return 0.0\n",
    "        X = np.array([x for (x, y) in dataset])\n",
    "        y = np.array([y for (x, y) in dataset])\n",
    "        preds = h.predict(X)\n",
    "        return np.mean(preds != y)\n",
    "\n",
    "    def algorithmA2(self, X_stream, oracle, verbose = False):\n",
    "        self.S = []\n",
    "        self.T = []\n",
    "        n_infered = 0  # compteur labels infÃ©rÃ©s\n",
    "        self.n_requetes_oracle = 0\n",
    "\n",
    "        for n, x_n in enumerate(X_stream, start=1):\n",
    "            ## STEP 1 : Calcul des deux hypothÃ¨ses candidates\n",
    "            # â€œTrain hâº and hâ» using candidate labels +1 and -1â€\n",
    "\n",
    "            # Pour chaque point x_n, vous construisez deux hypothÃ¨ses :\n",
    "            #   h_pos = hypothÃ¨se entraÃ®nÃ©e sur S âˆª T avec (x_n, +1) ajoutÃ© dans S,\n",
    "            #   h_neg = hypothÃ¨se entraÃ®nÃ©e sur S âˆª T avec (x_n, âˆ’1) ajoutÃ© dans S.\n",
    "\n",
    "            #h_pos = self.learn_H(self.S + [(x_n, +1)], self.T)\n",
    "            #h_neg = self.learn_H(self.S + [(x_n, -1)], self.T)\n",
    "\n",
    "            ## CrÃ©ation des hypothÃ¨ses candidate\n",
    "            #       Ã§a ajout a S (x_n, y_hat) PUIS apprenant un classifieur\n",
    "            h_learn_pos = self.S + [(x_n, +1)]\n",
    "            h_learn_neg = self.S + [(x_n, -1)]\n",
    "            \n",
    "            h_pos = self.learn_H(h_learn_pos, self.T)\n",
    "            h_neg = self.learn_H(h_learn_neg, self.T)\n",
    "\n",
    "\n",
    "            ## STEP 2 : Erreurs empiriques associÃ©es\n",
    "            # definition des \"versions spaces\" restreints ( ou en anglais : version space disagreement )\n",
    "            err_pos = self.empirical_error(h_pos, self.S + self.T)\n",
    "            err_neg = self.empirical_error(h_neg, self.S + self.T)\n",
    "\n",
    "            ## STEP 3 : Calcul du seuil Delta Î”â‚™\n",
    "            Delta = self.delta_schedule(max(1, n - 1))\n",
    "\n",
    "            ## STEP 4 :  DÃ©cision dâ€™infÃ©rence\n",
    "            # â€œInfer the label if the difference in empirical errors exceeds the threshold; otherwise query.â€\n",
    "            inferred_label = None\n",
    "\n",
    "            # Si err_neg - err_pos > Î” â‡’ label infÃ©rÃ© = +1\n",
    "            if err_neg - err_pos > Delta:\n",
    "                inferred_label = +1\n",
    "\n",
    "            # Si err_pos - err_neg > Î” â‡’ label infÃ©rÃ© = âˆ’1\n",
    "            elif err_pos - err_neg > Delta:\n",
    "                inferred_label = -1\n",
    "\n",
    "            # Sinon â‡’ requÃªte Ã  lâ€™oracle\n",
    "\n",
    "            ## STEP 5 : Mise Ã  jour (infÃ©rence ou requÃªte)\n",
    "            # Si le label est infÃ©rÃ©, ajout dans S\n",
    "            if inferred_label is not None:\n",
    "                self.S.append((x_n, inferred_label))\n",
    "                n_infered += 1\n",
    "\n",
    "            # Sinon â†’ interrogation de l'oracle et ajout dans T\n",
    "            else:\n",
    "                y_n = oracle(x_n)\n",
    "                self.T.append((x_n, y_n))\n",
    "                self.n_requetes_oracle += 1\n",
    "\n",
    "            #  Affichage tous les 50 points\n",
    "            if verbose and n % 50 == 0:  # affichage tous les 50 points\n",
    "                print(f\"Point {n}: infÃ©rÃ©s {n_infered}, oracle {self.n_requetes_oracle}\")\n",
    "\n",
    "\n",
    "            ## STEP 6 : Construction de lâ€™hypothÃ¨se finale\n",
    "            # apprenez un dernier classifieur h_final basÃ© sur S âˆª T\n",
    "        h_final = self.learn_H(self.S, self.T)\n",
    "        return h_final, self.n_requetes_oracle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e532c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class SimpleClassifier:\n",
    "    \"\"\"\n",
    "    Un classifieur gÃ©nÃ©rique pouvant fonctionner en :\n",
    "    - mode constant (toujours prÃ©dire y = +1 ou -1),\n",
    "    - mode logistic regression binaire normal.\n",
    "\n",
    "    GÃ©re les 3 cas (garantit que lâ€™algorithme AÂ² ne plante jamais): \n",
    "        - Pas de donnÃ©es    Retourne un modÃ¨le qui prÃ©dit +1 pour tout \n",
    "        - Une seule classe  Retourne un modÃ¨le constant qui prÃ©dit ce label \n",
    "        - Deux classes      EntraÃ®ne une logistic regression normale\n",
    "    \"\"\"\n",
    "    def __init__(self, mode=\"constant\", constant_value=1, clf=None):\n",
    "        self.mode = mode                            # \"constant\" ou \"logreg\"\n",
    "        self.constant_value = constant_value        # utilisÃ© en mode constant\n",
    "        self.clf = clf                              # logistic regression entraÃ®nÃ©e\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        if self.mode == \"constant\":\n",
    "            return np.full(len(X), self.constant_value, dtype=int)\n",
    "\n",
    "        # Sinon => logistic regression\n",
    "        preds = self.clf.predict(X)\n",
    "        return np.where(preds == 0, -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6fe0f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_H_fn(S, T):\n",
    "    \"\"\"\n",
    "    GÃ¨re les 3 cas :\n",
    "        - S âˆª T = âˆ…\n",
    "        - une seule classe\n",
    "        - deux classes â†’ logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    ### STEP 1 : Fusion des donnÃ©es S et T (des listes de tuples (x, y)) pour avoir S âˆª T\n",
    "    data = S + T\n",
    "\n",
    "    ### STEP 2 : ## Cas 1 : aucune donnÃ©e disponible \n",
    "    # On ne peut pas entraÃ®ner un classifieur sans donnÃ©es, \n",
    "    # donc on renvoie donc un classifieur trivial qui prÃ©dit toujours +1.\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return SimpleClassifier(mode=\"constant\", constant_value=1)\n",
    "\n",
    "    # Extraction X et y\n",
    "    X = np.array([x for (x, y) in data])\n",
    "    y = np.array([y for (x, y) in data])\n",
    "\n",
    "    # Conversion des labels dans le format scikit-learn\n",
    "    y_sklearn = np.where(y == -1, 0, 1)\n",
    "\n",
    "    ## Cas 2 : une seule classe prÃ©sente \n",
    "    # La rÃ©gression logistique ne peut pas sâ€™entraÃ®ner avec une seule classe. \n",
    "    # Donc si toutes les donnÃ©es portent le mÃªme label, \n",
    "    # on retourne un classifieur constant qui prÃ©dit ce label.\n",
    "    unique = np.unique(y_sklearn)\n",
    "    \n",
    "    if len(unique) == 1:\n",
    "        const_value = -1 if unique[0] == 0 else 1\n",
    "        return SimpleClassifier(mode=\"constant\", constant_value=const_value)\n",
    "\n",
    "    ## Cas 3 : les deux classes sont prÃ©sentes â†’ apprentissage normal\n",
    "    clf = LogisticRegression(max_iter=200, solver=\"liblinear\")\n",
    "    clf.fit(X, y_sklearn)\n",
    "\n",
    "    return SimpleClassifier(mode=\"logreg\", clf=clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0c061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_passive(X_pool, y_pool, n_queries, clf_class, **clf_kwargs):\n",
    "    \"\"\"EntraÃ®ne un classifieur passif choisi sur n_queries exemples du pool\"\"\"\n",
    "    idx = np.random.choice(len(X_pool), size=n_queries, replace=False)\n",
    "    X_train, y_train = X_pool[idx], y_pool[idx]\n",
    "    y_train = np.where(y_train == 0, -1, y_train)\n",
    "    \n",
    "    clf = clf_class(**clf_kwargs)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91a261fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_passive_baseline(X_pool, y_pool, n_queries, learn_H):\n",
    "    \"\"\"\n",
    "    EntraÃ®ne un modÃ¨le passif sur n_queries exemples tirÃ©s au hasard.\n",
    "    Si n_queries = 0 â†’ retourne un classifieur constant +1.\n",
    "    \"\"\"\n",
    "    # STEP 0 : aucune requÃªte â†’ modÃ¨le trivial\n",
    "    if n_queries == 0:\n",
    "        return SimpleClassifier(mode=\"constant\", constant_value=1)\n",
    "\n",
    "    # STEP 1 : SÃ©lection alÃ©atoire de n_queries donnÃ©es\n",
    "    idx = np.random.choice(len(X_pool), size=n_queries, replace=False)\n",
    "    X_train = X_pool[idx]\n",
    "    y_train = y_pool[idx]\n",
    "\n",
    "    # STEP 2 : Remapping 0 â†’ -1 si nÃ©cessaire\n",
    "    y_train = np.where(y_train == 0, -1, y_train)\n",
    "\n",
    "    # STEP 3 : Format (x, y)\n",
    "    data = list(zip(X_train, y_train))\n",
    "\n",
    "    # STEP 4 : EntraÃ®nement via learn_H\n",
    "    model = learn_H(data, [])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bdaeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score\n",
    "\n",
    "\n",
    "def plot_dataset_visualizations(\n",
    "        X, y, \n",
    "        all_results,\n",
    "        X_test=None, y_test=None,\n",
    "        passive_preds=None,\n",
    "        X_train=None,\n",
    "        title=\"Visualisation AÂ²\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Produit les 6 visualisations pour un dataset,\n",
    "    en utilisant uniquement all_results[?] pour rÃ©cupÃ©rer :\n",
    "    - h_star\n",
    "    - S_labeled\n",
    "    - passive accuracies\n",
    "    - history\n",
    "    - dataset name\n",
    "    \"\"\"\n",
    "\n",
    "    # RÃ©cupÃ©ration : on suppose que la fonction est appelÃ©e juste aprÃ¨s avoir ajoutÃ© un rÃ©sultat\n",
    "    r = all_results[-1]\n",
    "    dataset_name = r[\"dataset\"]\n",
    "    A2_res = r[\"A2\"]\n",
    "    passive_results = r[\"passive\"]\n",
    "\n",
    "    h_star = A2_res.get(\"model\", None)\n",
    "    S_labeled = A2_res.get(\"S_labeled\", None)\n",
    "    history = A2_res.get(\"history\", None)\n",
    "    n_labels = len(S_labeled) if S_labeled is not None else 0\n",
    "    S_final = S_labeled if S_labeled is not None else []\n",
    "\n",
    "    # PCA si nÃ©cessaire\n",
    "    if X.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        X_vis = pca.fit_transform(X)\n",
    "        if X_test is not None:\n",
    "            X_test_vis = pca.transform(X_test)\n",
    "    else:\n",
    "        X_vis = X.copy()\n",
    "        if X_test is not None:\n",
    "            X_test_vis = X_test.copy()\n",
    "\n",
    "    fig, axes = plt.subplots(6, 2, figsize=(15, 25))  \n",
    "    ax = axes.ravel()\n",
    "\n",
    "\n",
    "\n",
    "    # Indices AÂ² labellisÃ©s\n",
    "    oracle_indices = []\n",
    "    if S_labeled is not None:\n",
    "        for tx, _ in S_labeled:\n",
    "            matches = np.where(np.all(X == tx, axis=1))[0]\n",
    "            oracle_indices.extend(matches)\n",
    "\n",
    "    # ===================================================\n",
    "    # 1) Points colorÃ©s selon label\n",
    "    # ===================================================\n",
    "    ax0 = ax[0]\n",
    "    ax0.scatter(X_vis[:,0], X_vis[:,1],\n",
    "                c=['red' if lab==0 else 'blue' for lab in y], s=20)\n",
    "    ax0.set_title(\"1) Points colorÃ©s selon labels\")\n",
    "    ax0.grid(True)\n",
    "\n",
    "    # ===================================================\n",
    "    # 2) Erreurs du learner â†’ matrice de confusion\n",
    "    # ===================================================\n",
    "    ax1 = ax[1]  # ton graphe 2\n",
    "    if h_star is not None and X_test is not None:\n",
    "        y_pred_test = h_star.predict(X_test)\n",
    "        y_pred_test = np.where(y_pred_test == -1, 0, y_pred_test)  # si labels -1/1\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred_test)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_test))\n",
    "        disp.plot(ax=ax1, cmap='Blues', colorbar=True)\n",
    "        ax1.set_title(\"2) Matrice de confusion du learner\")\n",
    "    else:\n",
    "        ax1.text(0.5,0.5,\"Pas de learner fourni\", ha='center')\n",
    "\n",
    "\n",
    "    # ===================================================\n",
    "    # 3) Bar plot accuracies\n",
    "    # ===================================================\n",
    "    ax2 = ax[2]\n",
    "    ax2.set_title(\"3) Test Accuracy Comparison\")\n",
    "    ax2.grid(False)\n",
    "\n",
    "    if h_star is not None and passive_results is not None:\n",
    "        acc_active = A2_res[\"accuracy\"]\n",
    "        labels = [\"AÂ² Active\"] + list(passive_results.keys())\n",
    "        accs = [acc_active] + [v[\"accuracy\"] for v in passive_results.values()]\n",
    "        ax2.bar(labels, accs)\n",
    "        ax2.set_ylim(0,1)\n",
    "        ax2.set_ylabel(\"Test Accuracy\")\n",
    "        ax2.tick_params(axis='x', rotation=30)\n",
    "    else:\n",
    "        ax2.text(0.5,0.5,\"Pas de rÃ©sultats\",ha='center')\n",
    "\n",
    "    # --------------------\n",
    "    # 4) Nombre d'Ã©chantillons utilisÃ©s / labellisÃ©s\n",
    "    # --------------------\n",
    "    ax[3].set_title(\"4) Nombre d'Ã©chantillons pour actif et passifs\")\n",
    "\n",
    "    # Nombre unique d'Ã©chantillons labellisÃ©s par AÂ²\n",
    "    unique_S = {tuple(xy[0]) for xy in S_labeled} if S_labeled is not None else set()\n",
    "    n_unique_A = len(unique_S)\n",
    "\n",
    "    labels = [\"AÂ² Active\"] + list(passive_preds.keys())\n",
    "    counts = [n_unique_A] + [len(X_train)] * len(passive_preds)  # taille train pour passifs\n",
    "\n",
    "    ax[3].bar(labels, counts, color=['green'] + ['blue']*len(passive_preds))\n",
    "    ax[3].set_ylabel(\"Nombre d'Ã©chantillons\")\n",
    "    ax[3].tick_params(axis='x', rotation=30)\n",
    "    ax[3].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "    # ===================================================\n",
    "    # 5) Points labellisÃ©s par AÂ²\n",
    "    # ===================================================\n",
    "    ax4 = ax[4]\n",
    "    ax4.scatter(X_vis[:,0], X_vis[:,1], c='lightgray', s=20)\n",
    "    if S_labeled is not None:\n",
    "        idxs = []\n",
    "        for tx,_ in S_labeled:\n",
    "            ii = np.where(np.all(X == tx, axis=1))[0]\n",
    "            idxs.extend(ii)\n",
    "        ax4.scatter(X_vis[idxs,0], X_vis[idxs,1],\n",
    "                    c='green', s=50)\n",
    "    ax4.set_title(\"5) Points labellisÃ©s par AÂ²\")\n",
    "    ax4.grid(True)\n",
    "\n",
    "    # ===================================================\n",
    "    # 6) Heatmap + Outliers + AÂ² queries (X,y bruts)\n",
    "    # ===================================================\n",
    "    ax5 = ax[5]  # le 6Ã¨me sous-graphique\n",
    "    ax5.set_title(\"6) Heatmap + Outliers + AÂ² Queries\")\n",
    "    ax5.grid(True)\n",
    "\n",
    "    # Couleurs et colormap\n",
    "    colors_map = ['Reds', 'Blues']\n",
    "    class_colors = ['red', 'blue']\n",
    "\n",
    "    # Heatmaps par classe et points\n",
    "    heatmaps, points = [], []\n",
    "    for label in np.unique(y):\n",
    "        X_class = X_vis[y==label]\n",
    "        kde = gaussian_kde(X_class.T)\n",
    "        x_min, x_max = X_class[:,0].min()-1, X_class[:,0].max()+1\n",
    "        y_min, y_max = X_class[:,1].min()-1, X_class[:,1].max()+1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min,x_max,100), np.linspace(y_min,y_max,100))\n",
    "        grid_coords = np.vstack([xx.ravel(), yy.ravel()])\n",
    "        density = kde(grid_coords).reshape(xx.shape)\n",
    "        ax5.contourf(xx, yy, density, levels=20, alpha=0.3, cmap=colors_map[label])\n",
    "        heatmaps.append(Patch(color=plt.cm.get_cmap(colors_map[label])(0.6), label=f\"Heatmap classe {label}\"))\n",
    "\n",
    "        # Points normaux\n",
    "        ax5.scatter(X_class[:,0], X_class[:,1], c=class_colors[label], s=20)\n",
    "        points.append(Line2D([0],[0], marker='o', color='w', label=f\"Points classe {label}\",\n",
    "                            markerfacecolor=class_colors[label], markersize=7))\n",
    "\n",
    "    # Outliers (5 par classe)\n",
    "    outlier_handles = []\n",
    "    for label in np.unique(y):\n",
    "        X_class_full = X_vis[y==label]\n",
    "        centroid = X_class_full.mean(axis=0)\n",
    "        distances = np.linalg.norm(X_class_full - centroid, axis=1)\n",
    "        out_idx = np.where(y==label)[0][np.argsort(distances)[-5:]]\n",
    "        ax5.scatter(X_vis[out_idx,0], X_vis[out_idx,1], facecolors='none', edgecolors='black', s=100)\n",
    "        outlier_handles.append(Line2D([0],[0], marker='o', color='w', label=f\"Outliers classe {label}\",\n",
    "                                    markerfacecolor='none', markeredgecolor='black', markersize=10))\n",
    "\n",
    "    # Points AÂ² (SVM margin)\n",
    "    svm = SVC(kernel='linear').fit(X_vis, y)\n",
    "    distances = svm.decision_function(X_vis)\n",
    "    n_queries = min(30, len(X_vis))\n",
    "    interesting_indices = np.argsort(np.abs(distances))[:n_queries]\n",
    "    ax5.scatter(X_vis[interesting_indices,0], X_vis[interesting_indices,1],\n",
    "                facecolors='none', edgecolors='green', s=100)\n",
    "    a2_legend = [Line2D([0],[0], marker='o', color='w', label='AÂ² query',\n",
    "                        markerfacecolor='none', markeredgecolor='green', markersize=10)]\n",
    "\n",
    "    # LÃ©gende complÃ¨te\n",
    "    ax5.legend(handles=heatmaps + points + outlier_handles + a2_legend,\n",
    "            bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "\n",
    "    # --------------------\n",
    "    # 7) PrÃ©cision / rappel vs nombre de labels\n",
    "    # --------------------\n",
    "    if n_labels > 0 and X_test is not None:\n",
    "        y_pred_test = h_star.predict(X_test)\n",
    "        y_pred_test = np.where(y_pred_test == -1, 0, y_pred_test)  # ajuster les labels si nÃ©cessaire\n",
    "\n",
    "        average_type = 'binary' if len(np.unique(y_test)) == 2 else 'macro'\n",
    "        precision_a2 = precision_score(y_test, y_pred_test, average=average_type)\n",
    "        recall_a2 = recall_score(y_test, y_pred_test, average=average_type)\n",
    "\n",
    "        metrics = [\"Precision\", \"Recall\"]\n",
    "        x = np.arange(len(metrics))  # positions [0,1]\n",
    "        width = 0.15  # largeur des barres\n",
    "        offset = 0\n",
    "\n",
    "        # Barre AÂ²\n",
    "        unique_S = {tuple(xy[0]) for xy in S_labeled} if S_labeled is not None else set()\n",
    "        n_unique_A = len(unique_S)\n",
    "        ax[6].bar(x + offset, [precision_a2, recall_a2], width=width, color='green', alpha=0.6,\n",
    "                label=f\"AÂ² ({n_unique_A} labels)\")\n",
    "        offset += width\n",
    "\n",
    "        # Barres passives\n",
    "        if passive_preds is not None:\n",
    "            passive_colors = ['blue', 'orange', 'purple', 'brown', 'cyan']\n",
    "            for i, (name, preds) in enumerate(passive_preds.items()):\n",
    "                preds_adj = np.where(preds < 0, 0, preds)\n",
    "                p = precision_score(y_test, preds_adj, average=average_type)\n",
    "                r = recall_score(y_test, preds_adj, average=average_type)\n",
    "                ax[6].bar(x + offset, [p, r], width=width, alpha=0.7, color=passive_colors[i % len(passive_colors)],\n",
    "                        label=name)\n",
    "                offset += width\n",
    "\n",
    "        # Centrer les labels x\n",
    "        ax[6].set_xticks(x + width * (len(passive_preds)+1)/2)\n",
    "        ax[6].set_xticklabels(metrics)\n",
    "        ax[6].legend()\n",
    "        ax[6].set_title(\"7) Rappel/Precision en fonction du nombre de labels\")\n",
    "\n",
    "\n",
    "    # ===================================================\n",
    "    # 8) Convergence d'AÂ²\n",
    "    # ===================================================\n",
    "    ax7 = ax[7]\n",
    "    if history is not None and \"iters\" in history:\n",
    "        metrics = [h[\"metric\"] for h in history[\"iters\"] if \"metric\" in h]\n",
    "        if len(metrics):\n",
    "            ax7.plot(metrics)\n",
    "            ax7.set_xlabel(\"Steps\")\n",
    "            ax7.set_ylabel(\"metric = DISAG *(UB-LB)\")\n",
    "            ax7.set_title(\"8) Convergence d'AÂ²\")\n",
    "        else:\n",
    "            ax7.text(0.5,0.5,\"Aucune mÃ©trique\",ha='center')\n",
    "    else:\n",
    "        ax7.text(0.5,0.5,\"Pas d'historique\",ha='center')\n",
    "    ax7.grid(True)\n",
    "\n",
    "    # ===================================================\n",
    "    # 9) Comparaison des erreurs AÂ² vs passifs\n",
    "    # ===================================================\n",
    "    ax[8].set_title(\"9) Comparaison des erreurs\")\n",
    "    if h_star is not None and X_test is not None:\n",
    "        y_pred_test = h_star.predict(X_test)\n",
    "        y_pred_test = np.where(y_pred_test == -1, 0, y_pred_test)\n",
    "\n",
    "        # Points AÂ² corrects / incorrects\n",
    "        correct_idx = y_pred_test == y_test\n",
    "        incorrect_idx = y_pred_test != y_test\n",
    "        ax[8].scatter(X_test_vis[correct_idx,0], X_test_vis[correct_idx,1],\n",
    "                    color='green', s=40)\n",
    "        ax[8].scatter(X_test_vis[incorrect_idx,0], X_test_vis[incorrect_idx,1],\n",
    "                    color='red', s=40)\n",
    "\n",
    "        # Handles pour la lÃ©gende\n",
    "        handles = [\n",
    "            Line2D([0],[0], marker='o', color='w', markerfacecolor='green', markersize=7, label='AÂ² Correct'),\n",
    "            Line2D([0],[0], marker='o', color='w', markerfacecolor='red', markersize=7, label='AÂ² Incorrect')\n",
    "        ]\n",
    "\n",
    "        # Passifs\n",
    "        if passive_preds is not None:\n",
    "            passive_colors = ['blue', 'orange', 'purple', 'brown', 'cyan']  # pour diffÃ©rencier les classifieurs\n",
    "            for i, (name, preds) in enumerate(passive_preds.items()):\n",
    "                color = passive_colors[i % len(passive_colors)]\n",
    "                preds_adj = np.where(preds < 0, 0, preds)\n",
    "                incorrect_passive = preds_adj != y_test\n",
    "                if np.any(incorrect_passive):\n",
    "                    ax[8].scatter(X_test_vis[incorrect_passive,0], X_test_vis[incorrect_passive,1],\n",
    "                                facecolors='none', edgecolors=color, s=50)\n",
    "                # CrÃ©e un handle correct pour la lÃ©gende\n",
    "                handles.append(Line2D([0],[0], marker='o', color='w',\n",
    "                                    markerfacecolor='none', markeredgecolor=color,\n",
    "                                    markersize=7, label=f'{name} Incorrect'))\n",
    "\n",
    "        ax[8].legend(handles=handles, loc='upper right')\n",
    "    else:\n",
    "        ax[8].text(0.5,0.5,\"Pas de learner ou test set\", ha='center')\n",
    "    ax[8].grid(True)\n",
    "\n",
    "    # ===================================================\n",
    "    # 10) Erreurs du learner\n",
    "    # ===================================================\n",
    "    ax9 = ax[9]\n",
    "    if h_star is not None and X_test is not None:\n",
    "        y_pred_test = h_star.predict(X_test)\n",
    "        y_pred_test = np.where(y_pred_test == -1, 0, y_pred_test)  # si labels -1/1\n",
    "        correct = y_pred_test == y_test\n",
    "\n",
    "        ax9.scatter(X_test_vis[correct,0], X_test_vis[correct,1],\n",
    "            c='green', s=20, label=\"Correct\")\n",
    "        ax9.scatter(X_test_vis[~correct,0], X_test_vis[~correct,1],\n",
    "                    c='red', s=40, label=\"Erreur\")\n",
    "\n",
    "        ax9.legend()\n",
    "    else:\n",
    "        ax9.text(0.5,0.5,\"Pas de learner fourni\",ha='center')\n",
    "    ax9.set_title(\"10) Erreurs du learner\")\n",
    "    ax9.grid(True)\n",
    "\n",
    "\n",
    "    # ===================================================\n",
    "    # 11) Comparatif des temps d'exÃ©cution\n",
    "    # ===================================================\n",
    "    ax10 = ax[10]\n",
    "    ax10.set_title(\"11) Temps d'exÃ©cution (s) par mÃ©thode\")\n",
    "\n",
    "    times = [A2_res.get(\"time\", 0)]\n",
    "    labels = [\"AÂ²\"]\n",
    "\n",
    "    # Ajouter les temps des mÃ©thodes passives\n",
    "    for clf_name, res in passive_results.items():\n",
    "        labels.append(clf_name)\n",
    "        times.append(res.get(\"time\", 0))\n",
    "\n",
    "    ax10.bar(labels, times)\n",
    "    ax10.set_ylabel(\"Temps (secondes)\")\n",
    "    ax10.set_xticks(np.arange(len(labels)))\n",
    "    ax10.set_xticklabels(labels, rotation=45)\n",
    "    ax10.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.suptitle(f\"{title} â€“ {dataset_name}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67774870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "def run_experiment(X, y, test_size=0.3, stream_seed=1):\n",
    "    \n",
    "    data = prepare_data_for_A2(X, y, test_size, stream_seed)\n",
    "\n",
    "    X_stream = data[\"X_stream\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_pool_arr = data[\"X_pool_arr\"]\n",
    "    y_pool_arr = data[\"y_pool_arr\"]\n",
    "    oracle = data[\"oracle\"]\n",
    "\n",
    "    active = A2ActiveLearner(learn_H_fn, delta_schedule)\n",
    "    h_active, n_requetes_oracle = active.algorithmA2(X_stream, oracle, True)\n",
    "\n",
    "    # ------------------------\n",
    "    # DÃ©finition des classifieurs passifs\n",
    "    # ------------------------\n",
    "    passive_classifiers = {\n",
    "        \"Naive Bayes\": GaussianNB,\n",
    "        \"kNN\": lambda: KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Random Forest\": lambda: RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "        \"SVM\": lambda: SVC(kernel='linear', C=1.0),\n",
    "        \"Logistic Regression\": lambda: LogisticRegression(max_iter=200, solver='liblinear')\n",
    "    }\n",
    "\n",
    "    passive_results = {}\n",
    "    for name, clf_fn in passive_classifiers.items():\n",
    "        X_train = X_pool_arr\n",
    "        y_train_mapped = np.where(y_pool_arr == 0, -1, y_pool_arr)\n",
    "\n",
    "        clf = clf_fn() if callable(clf_fn) else clf_fn\n",
    "        clf.fit(X_train, y_train_mapped)\n",
    "        preds = clf.predict(X_test)\n",
    "        passive_results[name] = {\n",
    "            \"accuracy\": accuracy_score(np.where(y_test==0,-1,y_test), preds),\n",
    "            \"recall\": recall_score(np.where(y_test==0,-1,y_test), preds),\n",
    "            \"f1\": f1_score(np.where(y_test==0,-1,y_test), preds)\n",
    "        }\n",
    "\n",
    "\n",
    "    # Accuracies et mÃ©triques pour actif\n",
    "    y_test_mapped = np.where(y_test==0, -1, y_test)\n",
    "    preds_active = h_active.predict(X_test)\n",
    "    metrics_active = {\n",
    "        \"accuracy\": accuracy_score(y_test_mapped, preds_active),\n",
    "        \"recall\": recall_score(y_test_mapped, preds_active),\n",
    "        \"f1\": f1_score(y_test_mapped, preds_active)\n",
    "    }\n",
    "\n",
    "    # Meilleur classifieur passif\n",
    "    best_passive_name = max(passive_results, key=lambda k: passive_results[k][\"accuracy\"])\n",
    "    metrics_passive = passive_results[best_passive_name]\n",
    "\n",
    "    # CrÃ©er le dictionnaire res complet\n",
    "    res = {\n",
    "        \"metrics_active\": metrics_active,\n",
    "        \"metrics_passive\": metrics_passive,\n",
    "        \"best_passive\": best_passive_name,\n",
    "        \"queries\": n_requetes_oracle,\n",
    "        \"n_pool\": len(X_pool_arr),\n",
    "        \"n_test\": len(X_test),\n",
    "        \"passive_classifiers\": passive_results\n",
    "    }\n",
    "\n",
    "    # ------------------------\n",
    "    # Affichage de tous les graphiques\n",
    "    # ------------------------\n",
    "    # PrÃ©parer les prÃ©dictions passives pour le plot\n",
    "    passive_preds = {name: clf_fn().fit(X_pool_arr, np.where(y_pool_arr == 0, -1, y_pool_arr)).predict(X_test)\n",
    "                    for name, clf_fn in passive_classifiers.items()}\n",
    "\n",
    "    # Appel corrigÃ© de plot_dataset_visualizations\n",
    "    plot_dataset_visualizations(\n",
    "        X, \n",
    "        y, \n",
    "        res,\n",
    "        X_test=X_test, \n",
    "        y_test=y_test,\n",
    "        passive_preds=passive_preds,   # ici on passe les prÃ©dictions des classifieurs passifs\n",
    "        X_train=X_pool_arr,\n",
    "        title=\"Visualisation AÂ²\"\n",
    "    )\n",
    "\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5fe7b",
   "metadata": {},
   "source": [
    "## CrÃ©ation de donnÃ©es synthÃ©tiques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe0e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# ParamÃ¨tres variÃ©s pour chaque dataset\n",
    "dataset_params = [\n",
    "    {\"name\": \"Easy\", \"n_features\": 5, \"n_informative\": 5, \"n_redundant\":0, \"flip_y\":0.0, \"class_sep\":2.5},\n",
    "    {\"name\": \"Medium\", \"n_features\": 10, \"n_informative\": 6, \"n_redundant\":3, \"flip_y\":0.05, \"class_sep\":1.5},\n",
    "    {\"name\": \"Hard\", \"n_features\": 20, \"n_informative\": 10, \"n_redundant\":5, \"flip_y\":0.1, \"class_sep\":1.0},\n",
    "    {\"name\": \"Noisy\", \"n_features\": 15, \"n_informative\": 5, \"n_redundant\":5, \"flip_y\":0.2, \"class_sep\":1.0},\n",
    "    {\"name\": \"Separated\", \"n_features\": 10, \"n_informative\": 8, \"n_redundant\":1, \"flip_y\":0.0, \"class_sep\":3.0},\n",
    "]\n",
    "\n",
    "datasets_syn = []\n",
    "\n",
    "for params in dataset_params:\n",
    "    X, y = make_classification(\n",
    "        n_samples=600,\n",
    "        n_features=params[\"n_features\"],\n",
    "        n_informative=params[\"n_informative\"],\n",
    "        n_redundant=params[\"n_redundant\"],\n",
    "        n_clusters_per_class=2,\n",
    "        flip_y=params[\"flip_y\"],\n",
    "        class_sep=params[\"class_sep\"],\n",
    "        random_state=42\n",
    "    )\n",
    "    datasets_syn.append({\"name\": params[\"name\"], \"X\": X, \"y\": y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8d9a5",
   "metadata": {},
   "source": [
    "## Importation de dataset : breast cancer \n",
    "\n",
    "*(https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35604273",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_real = []\n",
    "\n",
    "# Breast Cancer (2 classes)\n",
    "bc = load_breast_cancer()\n",
    "datasets_real.append({\n",
    "    \"name\": \"BreastCancer\",\n",
    "    \"X\": bc.data,\n",
    "    \"y\": bc.target\n",
    "})\n",
    "\n",
    "# Wine (filtrÃ© pour 2 classes)\n",
    "wine = load_wine()\n",
    "mask = wine.target != 2  # garder seulement classes 0 et 1\n",
    "datasets_real.append({\n",
    "    \"name\": \"Wine\",\n",
    "    \"X\": wine.data[mask],\n",
    "    \"y\": wine.target[mask]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf5e6d3",
   "metadata": {},
   "source": [
    "# Base de donnÃ©e complÃ¨te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa94d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = datasets_syn + datasets_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659a7f0",
   "metadata": {},
   "source": [
    "## Run les algos : AÂ² et Passif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a913779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running experiment on Easy ---\n",
      "Point 50: infÃ©rÃ©s 11, oracle 39\n",
      "Point 100: infÃ©rÃ©s 20, oracle 80\n",
      "Point 150: infÃ©rÃ©s 23, oracle 127\n",
      "Point 200: infÃ©rÃ©s 27, oracle 173\n",
      "Point 250: infÃ©rÃ©s 28, oracle 222\n",
      "Point 300: infÃ©rÃ©s 28, oracle 272\n",
      "Point 350: infÃ©rÃ©s 31, oracle 319\n",
      "Point 400: infÃ©rÃ©s 34, oracle 366\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m X, y = ds[\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m], ds[\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Running experiment on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m res = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m generated_results.append({\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: name, **res})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(X, y, test_size, stream_seed)\u001b[39m\n\u001b[32m     71\u001b[39m passive_preds = {name: clf_fn().fit(X_pool_arr, np.where(y_pool_arr == \u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m, y_pool_arr)).predict(X_test)\n\u001b[32m     72\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m name, clf_fn \u001b[38;5;129;01min\u001b[39;00m passive_classifiers.items()}\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Appel corrigÃ© de plot_dataset_visualizations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[43mplot_dataset_visualizations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassive_preds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassive_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# ici on passe les prÃ©dictions des classifieurs passifs\u001b[39;49;00m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_pool_arr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVisualisation AÂ²\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mplot_dataset_visualizations\u001b[39m\u001b[34m(X, y, all_results, X_test, y_test, passive_preds, X_train, title)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03mProduit les 6 visualisations pour un dataset,\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03men utilisant uniquement all_results[?] pour rÃ©cupÃ©rer :\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \u001b[33;03m- dataset name\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# RÃ©cupÃ©ration : on suppose que la fonction est appelÃ©e juste aprÃ¨s avoir ajoutÃ© un rÃ©sultat\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m r = \u001b[43mall_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     31\u001b[39m dataset_name = r[\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     32\u001b[39m A2_res = r[\u001b[33m\"\u001b[39m\u001b[33mA2\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: -1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1) ExÃ©cuter les expÃ©riences sur tous les datasets\n",
    "# ----------------------------\n",
    "generated_results = []\n",
    "\n",
    "for ds in datasets:\n",
    "    name = ds[\"name\"]\n",
    "    X, y = ds[\"X\"], ds[\"y\"]\n",
    "    print(f\"\\n--- Running experiment on {name} ---\")\n",
    "    \n",
    "    res = run_experiment(X, y, test_size=0.3, stream_seed=3)\n",
    "    \n",
    "    generated_results.append({\"dataset\": name, **res})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) CrÃ©er un DataFrame rÃ©capitulatif\n",
    "# ----------------------------\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        \"dataset\": r[\"dataset\"],\n",
    "        \"acc_active\": r[\"metrics_active\"][\"accuracy\"],\n",
    "        \"acc_passive\": r[\"metrics_passive\"][\"accuracy\"],\n",
    "        \"queries\": r[\"queries\"]\n",
    "    }\n",
    "    for r in generated_results\n",
    "])\n",
    "\n",
    "print(\"\\n\\nResults summary:\")\n",
    "print(results_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbe9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) DÃ©tails par dataset\n",
    "# ----------------------------\n",
    "for r in generated_results:\n",
    "    print(f\"\\nDataset: {r['dataset']}\")\n",
    "    \n",
    "    # Metrics actif\n",
    "    metrics_a = r['metrics_active']\n",
    "    print(f\"AÂ² Active â€” Accuracy: {metrics_a['accuracy']:.3f}, Recall: {metrics_a['recall']:.3f}, F1: {metrics_a['f1']:.3f}\")\n",
    "    \n",
    "    # Metrics passif (meilleur)\n",
    "    metrics_p = r['metrics_passive']\n",
    "    print(f\"Max Passive ({r['best_passive']}) â€” Accuracy: {metrics_p['accuracy']:.3f}, Recall: {metrics_p['recall']:.3f}, F1: {metrics_p['f1']:.3f}\")\n",
    "    \n",
    "    # Nombre de requÃªtes\n",
    "    print(f\"Number of queries: {r['queries']}\")\n",
    "    \n",
    "    # DÃ©tails autres classifieurs passifs\n",
    "    print(\"Other passive classifiers:\")\n",
    "    for clf_name, metrics in r['passive_classifiers'].items():\n",
    "        print(f\"  {clf_name} â€” Accuracy: {metrics['accuracy']:.3f}, Recall: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b1838",
   "metadata": {},
   "source": [
    "# Maintentant analysons : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9e7e5",
   "metadata": {},
   "source": [
    "## Graphe 1 : Accuracy Active AÂ² vs Passive Baseline\n",
    "\n",
    "Objectif : montrer si AÂ² amÃ©liore le classifieur passif en utilisant moins de labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdeff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RÃ©cupÃ©rer les noms de tous les datasets et rÃ©sultats\n",
    "datasets = [r['dataset'] for r in generated_results]\n",
    "all_results = generated_results\n",
    "\n",
    "# RÃ©cupÃ©rer les noms des classifieurs passifs\n",
    "passive_clf_names = list(all_results[0]['passive_classifiers'].keys())\n",
    "all_clf_names = passive_clf_names + [\"AÂ² Active\"]\n",
    "\n",
    "# Construire les listes d'accuracies pour chaque classifieur\n",
    "accuracy_dict = {clf_name: [] for clf_name in all_clf_names}\n",
    "\n",
    "for r in all_results:\n",
    "    # accuracies passives\n",
    "    for clf_name in passive_clf_names:\n",
    "        accuracy_dict[clf_name].append(r['passive_classifiers'][clf_name]['accuracy'])\n",
    "    # accuracy AÂ² Active\n",
    "    accuracy_dict[\"AÂ² Active\"].append(r['metrics_active']['accuracy'])\n",
    "\n",
    "# TracÃ©\n",
    "plt.figure(figsize=(12, 6))\n",
    "for clf_name, acc_list in accuracy_dict.items():\n",
    "    plt.plot(datasets, acc_list, marker='o', label=clf_name)\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparaison AÂ² Active et toutes les mÃ©thodes passives\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9de52f",
   "metadata": {},
   "source": [
    "## 2) Graphe 2 : Nombre de requÃªtes AÂ² par dataset\n",
    "\n",
    "Objectif : illustrer le coÃ»t en requÃªtes (labels demandÃ©s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6aafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = results_df[\"queries\"].tolist()\n",
    "datasets = results_df[\"dataset\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(datasets, queries, color='skyblue')\n",
    "plt.ylabel(\"Number of oracle queries\")\n",
    "plt.title(\"Oracle query cost of AÂ² by dataset\")\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e3fb61",
   "metadata": {},
   "source": [
    "## Graphe 3 : Accuracy des classifieurs passifs (comparatif multi-modÃ¨les)\n",
    "\n",
    "Objectif : montrer que les mÃ©thodes passives sophistiquÃ©es (RF, SVM, kNN) font souvent mieux quâ€™AÂ².\n",
    "Cela appuie la conclusion : AÂ² nâ€™est pas destinÃ© Ã  battre les meilleurs classifieurs passifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4725de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Liste des datasets et rÃ©sultats\n",
    "datasets = [r[\"dataset\"] for r in generated_results]\n",
    "all_results = [r for r in generated_results]\n",
    "\n",
    "# Noms des classifieurs passifs (on prend ceux du premier dataset)\n",
    "passive_clf_names = list(all_results[0]['passive_classifiers'].keys())\n",
    "clf_names = passive_clf_names + [\"AÂ² Active\"]\n",
    "n_clfs = len(clf_names)\n",
    "\n",
    "x = np.arange(n_clfs)\n",
    "width = 0.8 / len(datasets)  # largeur des barres adaptÃ©e au nombre de datasets\n",
    "\n",
    "plt.figure(figsize=(max(12, n_clfs*1.2), 6))\n",
    "\n",
    "# GÃ©nÃ©ration de couleurs alÃ©atoires\n",
    "def random_color():\n",
    "    return (random.random(), random.random(), random.random())\n",
    "\n",
    "dataset_colors = [random_color() for _ in datasets]  # pour passifs\n",
    "a2_colors = [random_color() for _ in datasets]       # pour AÂ² Active\n",
    "\n",
    "# Plot des barres pour chaque dataset\n",
    "for i, (dataset_name, res) in enumerate(zip(datasets, all_results)):\n",
    "    accs = [res['passive_classifiers'][clf]['accuracy'] for clf in passive_clf_names] + [res['metrics_active']['accuracy']]\n",
    "    colors = [dataset_colors[i]]*len(passive_clf_names) + [a2_colors[i]]\n",
    "    plt.bar(x + i*width, accs, width, color=colors)\n",
    "\n",
    "plt.xticks(x + width*(len(datasets)-1)/2, clf_names, rotation=45)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Comparaison des classifieurs passifs et AÂ² Active sur tous les datasets\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# CrÃ©ation de la lÃ©gende personnalisÃ©e\n",
    "legend_elements = []\n",
    "for i, dataset_name in enumerate(datasets):\n",
    "    legend_elements.append(Patch(facecolor=dataset_colors[i], label=f\"{dataset_name} passifs\"))\n",
    "    legend_elements.append(Patch(facecolor=a2_colors[i], label=f\"{dataset_name} AÂ² Active\"))\n",
    "\n",
    "plt.legend(handles=legend_elements, bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
